<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0d0d0d;
            color: #ffffff;
            height: 100vh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        /* Voice Orb Animation */
        .orb-container {
            position: relative;
            width: 120px;
            height: 120px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: transform 0.3s ease;
        }

        .orb-container:hover {
            transform: scale(1.05);
        }

        .orb {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: linear-gradient(135deg, #4285f4, #9b72cb, #d96570);
            background-size: 200% 200%;
            animation: gradient-spin 5s ease infinite;
            filter: blur(10px);
            opacity: 0.8;
            transition: all 0.5s ease;
        }

        .orb-core {
            position: absolute;
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(5px);
            z-index: 10;
        }

        /* Active/Listening State */
        .orb-container.listening .orb {
            width: 120px;
            height: 120px;
            filter: blur(20px);
            opacity: 1;
            animation: gradient-spin 2s ease infinite, pulse-listening 2s infinite;
        }

        /* Processing State */
        .orb-container.processing .orb {
            width: 100px;
            height: 100px;
            filter: blur(15px);
            animation: gradient-spin 1s linear infinite;
        }

        @keyframes gradient-spin {
            0% {
                background-position: 0% 50%;
            }

            50% {
                background-position: 100% 50%;
            }

            100% {
                background-position: 0% 50%;
            }
        }

        @keyframes pulse-listening {
            0% {
                transform: scale(0.95);
                opacity: 0.8;
            }

            50% {
                transform: scale(1.1);
                opacity: 1;
            }

            100% {
                transform: scale(0.95);
                opacity: 0.8;
            }
        }

        /* Result Box Layout */
        .result-area {
            flex: 1;
            padding: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        textarea {
            width: 100%;
            height: 100%;
            max-width: 800px;
            background: rgba(28, 28, 30, 0.6);
            border: 1px solid #333;
            border-radius: 20px;
            padding: 24px;
            font-size: 24px;
            line-height: 1.6;
            color: #fff;
            resize: none;
            outline: none;
            font-family: 'Inter', sans-serif;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
        }

        textarea:focus {
            border-color: #0A84FF;
            background: rgba(28, 28, 30, 0.8);
        }

        textarea::placeholder {
            color: #555;
        }

        @keyframes fade-in {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Bottom Controls */
        .controls {
            padding: 40px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            background: linear-gradient(to top, #0d0d0d 80%, transparent);
        }

        select {
            background: #1c1c1e;
            color: white;
            border: 1px solid #333;
            padding: 8px 16px;
            border-radius: 20px;
            outline: none;
            cursor: pointer;
        }
    </style>
</head>

<body>

    <div class="result-area">
        <textarea id="resultBox" placeholder="Transcription will appear here..." readonly></textarea>
    </div>

    <div class="controls">
        <div id="orb" class="orb-container">
            <div class="orb"></div>
            <div class="orb-core"></div>
        </div>

        <div id="status" style="color: #666; font-size: 14px; min-height: 20px;">Ready</div>

        <select id="language">
            <option value="ne">Nepali (नेपाली)</option>
            <option value="hi" selected>Hindi (हिंदी)</option>
            <option value="mai">Maithili (मैथिली)</option>
        </select>
    </div>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js"></script>
    <script>
        // Explicitly set WASM paths for ONNX Runtime Web to load from local static directory
        ort.env.wasm.wasmPaths = {
            'ort-wasm.wasm': '/static/ort-wasm.wasm',
            'ort-wasm-simd.wasm': '/static/ort-wasm-simd.wasm',
            'ort-wasm-threaded.wasm': '/static/ort-wasm-threaded.wasm',
            'ort-wasm-simd-threaded.wasm': '/static/ort-wasm-simd-threaded.wasm'
        };
    </script>

    <script>
        const orb = document.getElementById('orb');
        const statusEl = document.getElementById('status');
        const resultBox = document.getElementById('resultBox');
        const langSelect = document.getElementById('language');

        let vadInstance = null;
        let isListening = false;

        // Audio Queue System
        let audioQueue = [];
        let isProcessing = false;

        orb.addEventListener('click', async () => {
            if (!isListening) {
                await startVAD();
            } else {
                await stopVAD();
            }
        });

        function updateStatus(text, type = 'normal') {
            statusEl.textContent = text;
            if (type === 'error') statusEl.style.color = '#FF453A';
            else if (type === 'active') statusEl.style.color = '#0A84FF';
            else statusEl.style.color = '#666';
        }

        async function startVAD() {
            try {
                updateStatus("Initializing...");

                vadInstance = await vad.MicVAD.new({
                    model: "v5", // Explicitly use Silero V5
                    // Tuned parameters for "Live" feel - More sensitive
                    positiveSpeechThreshold: 0.5, // Default is 0.5, we had 0.6. Back to 0.5 for better pickup.
                    negativeSpeechThreshold: 0.35, // Slightly lower to keep looking longer.
                    minSpeechFrames: 5,     // ~150ms 
                    redemptionFrames: 10,   // Reduced slightly to send chunks faster after speech ends
                    preSpeechPadFrames: 20,

                    onSpeechStart: () => {
                        console.log("Speech started");
                        updateStatus("Listening...", 'active');
                        orb.classList.add('listening');
                        orb.classList.remove('processing');
                    },
                    onSpeechEnd: (audio) => {
                        console.log("Speech ended");
                        // Transition visuals
                        orb.classList.remove('listening');
                        orb.classList.add('processing');

                        // Add to queue logic
                        if (audio.length > 0) {
                            audioQueue.push(audio);
                            processQueue();
                        }
                    },
                    onVADMisfire: () => {
                        console.log("VAD misfire");
                        orb.classList.remove('listening');
                    }
                });

                vadInstance.start();
                isListening = true;
                orb.classList.add('active');
                updateStatus("Listening...");
                resultBox.placeholder = "Listening...";

            } catch (err) {
                console.error("VAD init failed:", err);

                let errorMsg = err.message || err;

                // Specific check for SharedArrayBuffer/Headers issue
                if (!window.crossOriginIsolated) {
                    errorMsg = "Missing Security Headers (COOP/COEP). SharedArrayBuffer is not available.";
                }

                updateStatus(`Error: ${errorMsg}`, 'error');
            }
        }

        async function stopVAD() {
            if (vadInstance && isListening) {
                vadInstance.pause();
                isListening = false;
                orb.classList.remove('listening');
                orb.classList.remove('processing');
                orb.classList.remove('active');
                updateStatus("Ready");
                resultBox.placeholder = "Transcription will appear here...";
            }
        }

        async function processQueue() {
            if (isProcessing || audioQueue.length === 0) return;

            isProcessing = true;
            const audioFloat32 = audioQueue.shift();

            try {
                await processAudio(audioFloat32);
            } catch (err) {
                console.error("Queue processing error:", err);
            } finally {
                isProcessing = false;
                // Check if more items arrived while processing
                if (audioQueue.length > 0) {
                    processQueue();
                } else {
                    // Queue finished
                    orb.classList.remove('processing');
                    if (isListening) {
                        // Ensure we show listening state if still active
                        // But practically VAD handles the 'listening' class on speech start
                        // We just clean up 'processing' class
                    }
                }
            }
        }

        async function processAudio(audioFloat32) {
            const wavBlob = float32ToWav(audioFloat32);
            const formData = new FormData();
            formData.append("file", wavBlob, "recording.wav");
            const lang = langSelect.value;
            const url = `/transcribe?language=${lang}`;

            try {
                const response = await fetch(url, { method: 'POST', body: formData });
                if (response.ok) {
                    const data = await response.json();
                    if (data.transcription) {
                        const newText = data.transcription;
                        // Append to text box
                        if (resultBox.value.length > 0) {
                            resultBox.value += " " + newText;
                        } else {
                            resultBox.value = newText;
                        }
                        // Auto scroll to bottom
                        resultBox.scrollTop = resultBox.scrollHeight;
                    }
                } else {
                    console.error("Server error");
                }
            } catch (error) {
                console.error("Network error:", error);
            }
        }

        // --- WAV Helpers ---
        function float32ToWav(samples) {
            const buffer = new ArrayBuffer(44 + samples.length * 2);
            const view = new DataView(buffer);
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + samples.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, 16000, true);
            view.setUint32(28, 16000 * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, samples.length * 2, true);
            floatTo16BitPCM(view, 44, samples);
            return new Blob([view], { type: 'audio/wav' });
        }
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
        function floatTo16BitPCM(output, offset, input) {
            for (let i = 0; i < input.length; i++, offset += 2) {
                let s = Math.max(-1, Math.min(1, input[i]));
                s = s < 0 ? s * 0x8000 : s * 0x7FFF;
                output.setInt16(offset, s, true);
            }
        }
    </script>
</body>

</html>